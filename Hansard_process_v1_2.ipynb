{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "\n",
    "### Input parameters: file_in, file_out_suffix, date, lookup_file\n",
    "### file_in: name of file to run process on\n",
    "### file_out_suffix: short name identifier to append to file name: Hansard_v1_2_%s.txt\n",
    "### date: date to apply to debate; appears as field in output dataset\n",
    "###\n",
    "###\n",
    "### N.B. Location of MP lookup file and Stanfrod libraries are hard coded!!!\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Imports\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "# Stanford entity extraction full run - code taken from Four_entity_process_stanford_NER_v1_0.ipynb\n",
    "\n",
    "# Stanford imports\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "\n",
    "# Other imports\n",
    "import os\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# NLTK imports\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "\n",
    "import tkinter\n",
    "import operator\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "### Stanford paths ###\n",
    "\n",
    "#Set core path for Stanford NLP packages\n",
    "main_path = os.path.join(\"C:\\\\Users\\\\rothw\\\\Documents\\\\Python Scripts\\\\Python NLP\\\\StanfordNLP\\\\\",\n",
    "                         \"stanford-corenlp-full-2016-10-31\\\\\")\n",
    "# Set paths where the Standford NLP .jar files are located\n",
    "pathlist = [os.path.join(main_path,\"stanford-corenlp-3.7.0\"),\n",
    "            os.path.join(main_path,\"ner\\\\stanford-ner.jar\"),\n",
    "            os.path.join(main_path,\"postagger\\\\stanford-postagger.jar\")]\n",
    "###            os.path.join(main_path,\"parser\\\\stanford-parser.jar\"),\n",
    "###            os.path.join(main_path,\"parser\\\\stanford-parser-3.6.0-models.jar\"),\n",
    "# Set path to Stanford models\n",
    "mpath = [os.path.join(main_path,\"postagger\\\\models\"),\n",
    "         os.path.join(main_path,\"ner\\\\classifiers\")]\n",
    "# Set path to java.exe\n",
    "javapath = \"C:\\\\Program Files\\\\Java\\\\jre1.8.0_121\\\\bin\\\\java.exe\"\n",
    "\n",
    "# Add paths to the CLASSPATH environmental variable (as instructed by NLTK)\n",
    "os.environ['CLASSPATH'] = os.pathsep.join(pathlist)\n",
    "os.environ['STANFORD_MODELS'] = os.pathsep.join(mpath)\n",
    "os.environ['JAVAHOME'] = javapath\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Define function to tag NER sentence with BIO tags\n",
    "def stanfordNE2BIO(tagged_sent):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "    # Return BIO tagged sentence\n",
    "    return bio_tagged_sent\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "# Read in the lookup\n",
    "import csv\n",
    "with open('c:\\\\Users\\\\rothw\\\\Documents\\\\Python Scripts\\\\Python NLP\\\\mps.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    mp_list = list(reader)\n",
    "\n",
    "\n",
    "    \n",
    "# Split imported lookup into MP name and Consituency\n",
    "\n",
    "mp_names = []\n",
    "mp_consts = []\n",
    "\n",
    "for row in mp_list[1:]:\n",
    "#    mp_names.append(list(itertools.chain.from_iterable(row[1:3])))\n",
    "    temp = [row[1] + ' ' + row[2], 'MP']\n",
    "    mp_names.append(temp)\n",
    "    temp2 = [row[4], 'Constituency']\n",
    "    mp_consts.append(temp2)\n",
    "    \n",
    "\n",
    "# Append the two lists into one, tagging the type\n",
    "ents_lookup = mp_names + mp_consts\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main(file_in, file_out_suffix, date):\n",
    "    # Read in file\n",
    "    sample = open(file_in, 'r', encoding='utf8').read()\n",
    "\n",
    "\n",
    "    # Get title (first line) of doc - processed for stopwords and normalised\n",
    "    title = sample.split('\\n', 1)[0]\n",
    "\n",
    "    # Remove stopwords\n",
    "    title_trimmed = []\n",
    "\n",
    "    title_tokenized=nltk.word_tokenize(title)\n",
    "    title_trimmed = ' '.join([word for word in title_tokenized if word not in (stopwords.words('english'))])\n",
    "\n",
    "    print('\\n Title (title_trimmed): \\n')\n",
    "    print(title_trimmed)\n",
    "    \n",
    "    \n",
    "    # Cycle through the rows of the doc, to find MP names (speaking) and Constituency names (in the text)\n",
    "    ## N.b. only take Constituency names if they are in the conversation, not in announcing the speaker\n",
    "\n",
    "\n",
    "    # Split the whole text by newline\n",
    "    split_by_line = sample.split('\\n')\n",
    "\n",
    "    ents_identified = []\n",
    "    speaker=''\n",
    "\n",
    "\n",
    "    for row in split_by_line:\n",
    "        # only look for MPs, not Constituencies, in the speaker announcement\n",
    "        if len(row)<80:\n",
    "            for item in mp_names:\n",
    "                if item[0] in row:\n",
    "                    speaker = item\n",
    "                    speaker_spoken = [speaker, ['Speaking']]\n",
    "                    ents_identified.append(speaker_spoken)\n",
    "\n",
    "    # For all other rows, look for both MPs and Constituencies...\n",
    "    else:\n",
    "        for item in ents_lookup:\n",
    "            if item[0] in row:\n",
    "                speaker_spoken = [speaker, item]\n",
    "                ents_identified.append(speaker_spoken)\n",
    "                \n",
    "        ### ... and (v1.2) Stanford extracted entities ###\n",
    "        # Tokenize sentence with stanford NLP\n",
    "        tkn_sent = StanfordTokenizer().tokenize(row)\n",
    "        # Named entity tagging with stanford NLP\n",
    "        tag_sent = StanfordNERTagger('english.conll.4class.distsim.crf.ser.gz').tag(tkn_sent) \n",
    "        # Apply BIO tags to the tagged sentence\n",
    "        bio_tagged_sent = stanfordNE2BIO(tag_sent)\n",
    "        # Collate BIO parts of entities together\n",
    "        sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "        sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "\n",
    "        sent_conlltags = [(token, pos, ne) for token, pos, ne\n",
    "                     in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "        ne_tree = conlltags2tree(sent_conlltags)\n",
    "\n",
    "    \n",
    "        # Get entities from the trees\n",
    "        # ne_in_sent = []\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "                ne_label = subtree.label()\n",
    "                ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "                if ne_label in ['PERSON', 'LOCATION', 'ORGANIZATION']:\n",
    "                    speaker_spoken = [speaker, [ne_string, ne_label]]\n",
    "                    ents_identified.append(speaker_spoken)\n",
    "        \n",
    "\n",
    "    # Flatten the rows in the list\n",
    "\n",
    "    flattened = []\n",
    "\n",
    "    for row in ents_identified:\n",
    "        row_flattened = list(itertools.chain.from_iterable(row))\n",
    "        flattened.append(row_flattened)\n",
    "    \n",
    "    \n",
    "    # Pull together into a dataframe\n",
    "\n",
    "    output_df = pd.DataFrame(flattened, columns=[\"Speaker_name\", \"Speaker_type\", \"Subject_name\", \"Subject_type\"])\n",
    "    output_df['Debate'] = title_trimmed\n",
    "    output_df['Date'] = date\n",
    "\n",
    "    # Change column order\n",
    "    output_df = output_df[['Debate', 'Date', 'Speaker_name', 'Speaker_type', 'Subject_name', 'Subject_type']]\n",
    "\n",
    "    # Dedupe\n",
    "    output_dedup_df = output_df.drop_duplicates()        \n",
    "    \n",
    "    \n",
    "    # Export to file\n",
    "    output_dedup_df.to_csv('Hansard_v1_2_%s.txt' %file_out_suffix, sep=',', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
